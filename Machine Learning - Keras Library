Keras Library: Machine Learning


For Pre-Extracted Datasets: Dense Neural Networks
* values extracted from images

1) import

	import numpy as np
	from numpy import genfromtxt

2) get data as csv

	data = genfromtxt(“filepath”, delimiter=',')

3) labels indicated by last column in dataset (in this case 5th column)

	labels = data[:, 4]
		# says we want every row but only the 5th column

4) features indicated by everything except last columns

	features = data[:, 0:4]
		# says we want every row and column except for the 5th

5) by convention, X indicates features, y indicates labels

	X = features
	y = labels

6) split data between training set and test set

	from sklearn.model_selection import train_test_split
		#train_test_split splits up features and labels into test and training set + randomizes the shuffle

	X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)

7) scale data to fall within a certain range, this helps the neural network perform better

	from sklearn.preprocessing import MinMaxScaler

	scaler_object = MinMaxScaler()
		# finds min and max value, transforms array to fit
	scaler_object.fit(X_train)
		# fit object to training data; don’t pass test data, THATS CHEATING!

8) transform data

	scaled_X_train = scaler_object.transform(X_train)
	scaled_X_test = scaler_object.transform(X_test)

9) build neural network

	from keras.models import Sequential
	from keras.layers import Dense

	model = Sequential()
	# add in layers

	# 1 - Input Layer, expect 4 features (4 columns), input dimensions, relu = rectified linear unit
	model.add(Dense(4,input_dim=4,activation='relu'))

	# 2 - Hidden Layer, here is where you can play with neurons(too large or too small = bad results; 2x is great!)
	# no need for input dimension bc only 1 input
	model.add(Dense(8,activation='relu'))

	# 3 - Output Layer, sigmoid bc output will be between 0 and 1
	model.add(Dense(1, activation='sigmoid'))

10) compile the model

	model.compile(loss="binary_crossentropy", optimizer="adam", metrics=["accuracy"])
	
11) fit the model to data (train the model)

	model.fit(scaled_X_train, y_train, epochs=50, verbose=2)

13) predict on new unseen data

	from sklearn.metrics import confusion_matrix, classification_report

	predictions = (model.predict(X_test) > 0.5).astype("int32")
		# predict returns class probability instead of the class number so we have to perform the conversion manually

	confusion_matrix(y_test, predictions)
		# create confusion matrix with data; incorrect responses in upper right and lower left corners (?)

	print(classification_report(y_test,predictions))
		# get metric data

	model.save(“name_of_model.h5”)
		# save model

	from keras.models import load_model
		# load model
	new_model = load_model('name_of_model.h5')


For Raw Image Datasets: Convolutional Neural Networks

1) import 

	from keras.datasets import mnist
	import matplotlib.pyplot as plt
	import numpy as np

2) load data (in this case MNIST dataset)

	(x_train, y_train), (x_test, y_test) = mnist.load_data()
	# note: these dataset shapes are lacking a color channel atm

3) test data by grabbing a single image and display using matplotlib

	single_img = x_train[0]

	plt.imshow(single_img, cmap = “gray_r”)
	# note: add “_r” to end of cmap to reverse colors

4) convert labels to “one-hot encoding”

	from keras.utils.np_utils import to_categorical

	y_cat_test = to_categorical(y_test, 10)
	y_cat_train = to_categorical(y_train, 10)
	# 2nd param in to_categorical = number of possible classifications 

5) normalize values to be between 1 and 0

	x_train = x_train / x_train.max()
	x_test = x_test / x_test.max()
	# note: this is a basic way to do it. could also use .normalize()

6) reshape to include 4D color channel 

	x_train = x_train.reshape(60000, 28, 28, 1)
	x_test = x_test.reshape(10000, 28, 28, 1)

7) import materials for model, layers 

	from keras.models import Sequential
	from keras.layers import Dense, Conv2D, MaxPool2D, Flatten 

8) build model

	model = Sequential()
	# create Sequential Object

	model.add(Conv2D( filters=32, kernel_size=(4,4), input_shape=(28,28,1), activation='relu') )
	# Convolutional Layer (32 is good for simple, need more for more complex images)

	model.add(MaxPool2D(pool_size=(2,2) )
	# Pooling/Subsampling Layer
	# Read latest publications for best pool sizes

	model.add(Flatten( ) )
	# 2D—> 1D so Dense layer can understand

	model.add(Dense(128,activation='relu') ) 
	# Dense Layer
	# 128 neurons, encouraged to play with this number
	# common to increase by 2x (128, 256, 512..etc)

	model.add(Dense(10,activation='softmax'))
	# Output layer, classifier
	# output is locked in at 10


	model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'] )
	# Compile

9) get summary of layers, output shape and params

	model.summary()

10) train model 

	model.fit(x_train, y_cat_train, epochs=2)

11) evaluate model 

	model.evaluate(x_test, y_cat_test)

12) predict on new unseen data 

	from sklearn.metrics import classification_report

	predictions = np.argmax(model.predict(x_test), axis=-1)
	# for multi-class data must use ^
	# note: predictions results based on labels in orig. format (not one-hot encoded)

	print(classification_report(y_test, predictions))
	# thus, we pass in original label format for report
