Feature Matching

*feature matching - extracts defining key features from an input image using corner, edge and contour detection; then finds all the matches in a secondary image

* read in images as grayscale!


Brute Force Detection with ORB Descriptors

1. —> create detector object

		orb = cv2.ORB_create()

2. —> object takes image and mask, returns key points and descriptors

		kp1,des1 = orb.detectAndCompute(img1,None)
		kp2,des2 = orb.detectAndCompute(img2,None)

3. —> create Brute Force Matching object

		bf = cv2.BFMatcher(cv2.NORM_HAMMING,crossCheck=True)
		# NORM_HAMMING and crossCheck values are default params

4. —> check where matches occur

		matches = bf.match(des1,des2)
		# returns a tuple of match objects, each with distance attributes; less distance = better match

5. —> use sorted() to iterate sort by match.distance    

		matches = sorted(matches, key = lambda x:x.distance)

6. —> drawMatches sticks together query image and larger image

		img_matches = cv2.drawMatches(img1,kp1,img2,kp2,matches[:25],None, flags=2)
		# use slicing to limit amount of responses; in this case matches was 138


Brute Force Matching with SIFT Descriptors and Ratio Test
* Scale-Invariant Feature Transform
* much better at detection with images of different scales!

1. —> create SIFT object

		sift = cv2.SIFT_create()

2. —> object takes image and mask, returns keypoints and descriptors

		kp1,des1 = sift.detectAndCompute(img1,None)
		kp2,des2 = sift.detectAndCompute(img2,None)

3. —> create Brute Force Matching object

		bf = cv2.BFMatcher()

4. —> calculate matches, k value returns n best matches

		matches = bf.knnMatch(des1, des2, k=2)

5. —> apply ratio test to check for relative distance between matches

		good = [ ]

		for match1,match2 in matches:
			if match1.distance < 0.75 * match2.distance:
				good.append([match1])

6. —> draw matches

		sift_matches = cv2.drawMatchesKnn(img1,kp1,img2,kp2,good,None,flags=2)

FLANN Based Matcher
* Fast Library For Approximate Nearest Neighbor
* much faster, but finds general good matches over best matches

1. —> create detector object

	sift = cv2.SIFT_create()

2. —> object takes image and mask, returns keypoints and descriptors

		kp1,des1 = sift.detectAndCompute(img1,None)
		kp2,des2 = sift.detectAndCompute(img2,None)

3. —> create FLANN Matcher object:

		FLANN_INDEX_KDTREE = 0
		index_params = dict(algorithm=FLANN_INDEX_KDTREE,trees=5)
		search_params = dict(checks=50)

		flann = cv2.FlannBasedMatcher(index_params, search_params)

4. —> calculate matches, k value returns n best matches

		matches = flann.knnMatch(des1, des2, k=2)

5. —> apply ratio test to check for relative distance between matches

		good = [ ]

		for match1,match2 in matches:
			if match1.distance  <  0.7 * match2.distance:
				good.append([match1])

6. —> draw matches

		flann_matches = cv2.drawMatchesKnn(img1,kp1,img2,kp2,good,None,flags=2)


MAKING A MASK: w/ FLANN

* repeat above steps 1-4 (down to calculating matches!)

1. —> create a matches mask using list comprehension

		matchesMask = [[0,0] for i in range(len(matches))]
		# create a list of zero pairs!

2. —> if match is close enough, zero pair is changed to [1,0]

		for i,(match1,match2) in enumerate(matches):
			if match1.distance  <  0.7 * match2.distance:
				matchesMask[i] = [1,0]

3. —> format lines in drawMatches; flags = 0 every match, flags = 2 only lines

		draw_params = dict(matchColor = (0,255,0),
						singlePointColor = (255,0,0),
						matchesMask = matchesMask,
						flags=2)

4 —> draw matches, but with mask

	fann_matches = cv2.drawMatchesKnn(img1,kp1,img2,kp2,matches,None,**draw_params)
