Object Detection: Tensorflow + Python

documentation: https://tensorflow-object-detection-api-tutorial.readthedocs.io/en/latest/index.html

steps involved:

	- prepare images, using label-studio package

	- train deep learning model using Tensorflow Object Detection API
		-download pre-trained model and use transfer learning to train faster 

	- make real time detections using OpenCV + Python 

Setting Up Label Studio
	1. in terminal: pip install label-studio

	2. in terminal: label-studio start

	3. directory should end up with 2x the files
			
		- images	
		- annotations as .xml files

	4. split into test and train folders

		-make sure to include both files

Steps

	0. Setup Paths
		WORKSPACE_PATH = “Tensorflow/workspace”
		SCRIPTS_PATH = “Tensorflow/scripts”
		APIMODEL_PATH = “Tensorflow/models”
		ANNOTATION_PATH = WORKSPACE_PATH + “/annotations”
		IMAGE_PATH = WORKSPACE_PATH + “/images”
		MODEL_PATH = WORKSPACE_PATH + “/models”
 		PRETRAINED_MODEL_PATH = WORKSPACE_PATH + “/pre-trained-models”
		CONFIG_PATH = MODEL_PATH + “/my_ssd_mobnet/pipeline.config”
		CHECKPOINT_PATH = MODEL_PATH + “/my_ssd_mobnet”

	1. Create Label Map
		- representations of all the different objects contained in the images

		labels = [{’name”: Hello”, “id”: 1}, {“name”: Thanks”, “id”: 2}]

		label map must be in format of pbtxt

		with open(ANNOTATION_PATH + "/label_map.pbtxt", 'w') as file:
    			for label in labels:
        				file.write("item{\n")
        				file.write(f"\tname: '{label['name']}'\n")
        				file.write(f"\tid: {label['id']}\n")
        				file.write("}\n")
                   

	2. Create TF records
		-representation of our data used by Object Detection API
		
		- must use generate_tfrecord.py script —> official script for tensor flow object detection 
		
		*gets data in correct format!!!!

		!python {SCRIPTS_PATH + "/generate_tfrecord.py"} -x {IMAGE_PATH + "/train" } -l {ANNOTATION_PATH + "/label_map.pbtxt"} -o {ANNOTATION_PATH + "/train.record"}
		!python {SCRIPTS_PATH + "/generate_tfrecord.py"} -x {IMAGE_PATH + "/test" } -l {ANNOTATION_PATH + "/label_map.pbtxt"} -o {ANNOTATION_PATH + "/test.record"}
		
		*if error: module 'tensorflow' has no attribute 'gfile' —> find label_map_util.py —> replace: tf.gfile.GFile —> with: tf.gfile.io.GFile	
	
		*if error: incompatible constructor arguments —> open generate_tfrecord.py 
			—> replace: label_map_dict = label_map_util.get_label_map_dict(label_map) —> with: label_map_dict = label_map_util.get_label_map_dict(args.labels_path)

		* go to tensorflow —> workspace —> annotations
			check to see if we have labelmap, test and train files added!


	3. Download Pre-Trained TF Models

		in terminal (from tensorflow dir): git clone https://github.com/tensorflow/models

		or in IDE: !cd Tensorflow && git clone https://github.com/tensorflow/models

	4. Copy Model Config to Training Folder

		1. Get Model/Config
		For more models: https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2_detection_zoo.md
		
		- accuracy —> CoCo MAP (main average precision score): search highest
		- speed —> Speed(ms) : search lowest

		* must “untar” to use. paste in model URL into the following:
			*untar—> uncompress, .tar & .tar.gz are compression formats

		#wget.download('http://download.tensorflow.org/models/object_detection/tf2/20200711/ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8.tar.gz')
		#!mv ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8.tar.gz {PRETRAINED_MODEL_PATH}
		#!cd {PRETRAINED_MODEL_PATH} && tar -zxvf ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8.tar.gz


		* provided model: SSD MobileNet V2 FPNLite 320x320
			-optimized for speed!

		2. Copy Config To Training Folder

		* representation of the model
		1. find: pipeline.config
		2. copy into models folder 

			# create directory name
		CUSTOM_MODEL_NAME = 'my_ssd_mobnet' 
			# create directory
		!mkdir {'Tensorflow/workspace/models/‘+CUSTOM_MODEL_NAME}
			# copy config, place in created folder
		!cp {PRETRAINED_MODEL_PATH+'/ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8/pipeline.config'} {MODEL_PATH+'/'+CUSTOM_MODEL_NAME}

		

		
	5. Update Config for Transfer Learning
		
		1. Import Dependencies

		import tensorflow as tf
		from object_detection.utils import config_util
		from object_detection.protos import pipeline_pb2
		from google.protobuf import text_format

		2.Define Config Path + Tap into it
		
		CONFIG_PATH = MODEL_PATH+'/'+CUSTOM_MODEL_NAME+'/pipeline.config'
		config = config_util.get_configs_from_pipeline_file(CONFIG_PATH)

		*if ParseError —> visit pipeline.config copy —> remove fine_tune_checkpoint_version—> line 172

		type: config to pull up results

		3. Create A Config Shell

		pipeline_config = pipeline_pb2.TrainEvalPipelineConfig()
		with tf.io.gfile.GFile(CONFIG_PATH, 'r') as file:
    			proto_str = file.read()
    			text_format.Merge(proto_str, pipeline_config)
	
		pipeline_config.model.ssd.num_classes = 5
		pipeline_config.train_config.batch_size = 4

		# this is where we want our model to start from, transfer learning
		pipeline_config.train_config.fine_tune_checkpoint = PRETRAINED_MODEL_PATH + 'ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8/checkpoint/ckpt-0'

		# we want a detection-type model
		pipeline_config.train_config.fine_tune_checkpoint_type = 'detection'

		# where our label_map path is
		pipeline_config.train_input_reader.label_map_path = ANNOTATION_PATH + '/label_map.pbtxt'

		# where our tf_records are
		pipeline_config.train_input_reader.tf_record_input_reader.input_path[:] = [ANNOTATION_PATH + '/train.record']

		# for the testing component: where our label_map and tf_records are located
		pipeline_config.eval_input_reader[0].label_map_path = ANNOTATION_PATH + '/label_map.pbtxt'
		pipeline_config.eval_input_reader[0].tf_record_input_reader.input_path[:] = [ANNOTATION_PATH + '/train.record']


		4. Write Shell to Main Config
		
		config_text = text_format.MessageToString(pipeline_config)
			with tf.io.gfile.GFile(CONFIG_PATH, 'wb') as file:
   			 file.write(config_text)

				*wb = ‘write binary’

	6. Train Model

		in terminal cd to training_demo: 
		python model_main_tf2.py --model_dir=models/my_ssd_mobnet --pipeline_config_path=models/my_ssd_mobnet/pipeline.config
		
	7. Load Train Model from Checkpoint

	8. Detect in Real-Time

clear

